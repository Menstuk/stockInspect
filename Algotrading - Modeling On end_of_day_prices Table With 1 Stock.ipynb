{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0f0197",
   "metadata": {},
   "source": [
    "# Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import DataExplorer as De\n",
    "import DataPreprocessor as Dp\n",
    "import ModelActivator as Ma\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c87532d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62dc79ba"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6d70fb02d37c2fed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_by_id = pd.read_csv(\n",
    "'C:/Users/manor/Desktop/Final Project - Algotrading/First Developement Step/Database/stocks_by_id.csv',\n",
    "names=['id',\n",
    "       'stock'\n",
    "      ],\n",
    "header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_of_day_prices = pd.read_csv(\n",
    "        'C:/Users/manor/Desktop/Final Project - Algotrading/First Developement Step/Database/end_of_day_prices.csv',\n",
    "        names=['id',\n",
    "               'date',\n",
    "               'close_price'],\n",
    "header=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20b798f6a0c5c106"
  },
  {
   "cell_type": "markdown",
   "id": "6a3afd59",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c416e8",
   "metadata": {},
   "source": [
    "## IDs of all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7550351",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_by_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d44e7",
   "metadata": {},
   "source": [
    "## Closing price by stock id and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_day_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7083355",
   "metadata": {},
   "source": [
    "## We chose to use 'MMM' stock id: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e14a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm_stock_data = end_of_day_prices.loc[end_of_day_prices.id == 1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661b845",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stock Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70f58349d3455769"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mmm_stock_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfae4caf4896c285"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.style.use('fivethirtyeight')\n",
    "mmm_stock_data.plot(x='date', y='close_price', figsize=(30, 12), legend=False)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('closing price')\n",
    "plt.title('MMM stock data')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35c7d919a08e15bb"
  },
  {
   "cell_type": "markdown",
   "id": "1ffa7bb1",
   "metadata": {},
   "source": [
    "Some of the dates were missing, we filled them using the average of the past 2 days prior to the missing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fe186",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA = De.DataExplorer()\n",
    "\n",
    "# count number of NaN values\n",
    "nan_count = EDA.count_nan_values(mmm_stock_data)\n",
    "print(\"Number of NaN values in the table: \", nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of the missing_dates_detection function:\n",
    "gap_sizes_table, date_gaps, missing_dates = EDA.missing_dates_detection(mmm_stock_data)\n",
    "\n",
    "gap_sizes_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3173d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_of_gaps = pd.value_counts(gap_sizes_table['Gap Size']).values\n",
    "gap_sizes = pd.value_counts(gap_sizes_table['Gap Size']).keys()\n",
    "\n",
    "sns.barplot(x = gap_sizes, y = number_of_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_missing = ((missing_dates/(missing_dates + mmm_stock_data.shape[0])) * 100)\n",
    "\n",
    "print(\"Number of missing dates: \", missing_dates)\n",
    "print(f'Percentage of missing dates: {percentage_of_missing}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ee9bd8813ef461"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mmm_stock_data['date']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6570d3c845a45de9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocess = Dp.DataPreprocessor()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6699d9185bdf8f6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Amputation Method\n",
    "Some of the dates were missing. We wished to find the best filling method for them. We trained a model in order to do so. The best filling method was chosen based on the lowest MSE: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "596994c138378d6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5082b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_starting_params = {'n_estimators': 100, 'oob_score': True, 'random_state': 0}\n",
    "\n",
    "imputations_df = pd.DataFrame(columns=['Imputation Method', 'Dataset MSE'])\n",
    "\n",
    "methods_list = [1, 2, 4]\n",
    "\n",
    "for method in methods_list:\n",
    "    \n",
    "    #Fill each stock data with missing dates-prices by different method:price of the recent date/mean of prices of last 2\n",
    "    # dates/mean of prices of last 4 dates\n",
    "    raw_and_missing, missing = preprocess.add_missing_dates(mmm_stock_data, method)\n",
    "    \n",
    "    #Drop id column, set 'date' column as index and convert it to 'datetime' data type.\n",
    "    # This is done on the unified data, composed of the original dates-prices and artificially filled dates-prices\n",
    "    preprocess.alter_table(raw_and_missing, 'date', 'id')\n",
    "    \n",
    "    #create label column\n",
    "    raw_and_missing['tomorrow'] = raw_and_missing['close_price'].shift(-1)\n",
    "    \n",
    "    #split to X and y. Remove last row (its label is nan):\n",
    "    X, y = preprocess.rmv_nans_splt_Xy(raw_and_missing, 'tomorrow')\n",
    "    \n",
    "    #train random forest with starting parameters:\n",
    "    regr = RandomForestRegressor(**rf_starting_params)\n",
    "    \n",
    "    regr.fit(X, y)\n",
    "    \n",
    "    y_preds = regr.predict(X)\n",
    "    \n",
    "    stock_dataset_mse = mean_squared_error(y.values, y_preds)\n",
    "    \n",
    "    imputations_df.loc[len(imputations_df)] = pd.Series({'Imputation Method': method,\n",
    "                                                         'Dataset MSE': stock_dataset_mse\n",
    "                                                         })\n",
    "    \n",
    "\n",
    "best_stock_dataset_mse = imputations_df['Dataset MSE'].min()\n",
    "    \n",
    "best_row = imputations_df[imputations_df['Dataset MSE'] == best_stock_dataset_mse]\n",
    "\n",
    "best_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#fill missing dates by best method\n",
    "best_method_number = best_row['Imputation Method'].iloc[0]\n",
    "raw_and_missing, missing = preprocess.add_missing_dates(mmm_stock_data, best_method_number)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5157281008d125d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271527a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the raw data, the unified raw and for the missing data, remove stock id column,\n",
    "#set 'date' column as index and convert 'date' column to 'datetime' data type:\n",
    "preprocess.alter_table(mmm_stock_data, 'date', 'id')\n",
    "preprocess.alter_table(raw_and_missing, 'date', 'id')\n",
    "preprocess.alter_table(missing, 'date', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Number Of Lookback Features\n",
    "We trained the model with the same starting parameters in order to find the best number of lookback features (from 1 to 15).\n",
    "The data was temporally split to train (80%) and test (20%).<br>The best number of lookback features was chosen based on the lowest train MSE score. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b01b1811b0a56c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running random forest on multiple options of 'days before'(= n):\n",
    "\n",
    "days_before_list = list(range(1, 16))\n",
    "\n",
    "days_before_df = pd.DataFrame(columns=['days before', 'train MSE', 'test MSE'])\n",
    "\n",
    "# find the best 'n' number of days before based on lowest train mse\n",
    "for n in tqdm(days_before_list, desc='lookback days'):\n",
    "    \n",
    "    # create n lookback features\n",
    "    raw_and_missing_new = preprocess.create_features(raw_and_missing, n, 'close_price', '_days_ago', 'tomorrow')\n",
    "    \n",
    "    # drop all missing dates - since we want to prevent our model from learning the connections between each closing price\n",
    "    raw_and_missing_new =  preprocess.drop_missing_dates(raw_and_missing_new, missing)\n",
    "\n",
    "    ## Temporal splitting to train and test:\n",
    "    split_date = preprocess.generate_split_date(raw_and_missing_new) # train size was set to 0.8 of data size\n",
    "\n",
    "    train = raw_and_missing_new.iloc[raw_and_missing_new.index <= split_date].copy()\n",
    "    test = raw_and_missing_new.loc[raw_and_missing_new.index > split_date].copy()\n",
    "\n",
    "    # remove rows and split the data to X and y:\n",
    "    X_train, y_train = preprocess.rmv_nans_splt_Xy(train, 'tomorrow')\n",
    "    X_test, y_test = preprocess.rmv_nans_splt_Xy(test, 'tomorrow')\n",
    "    \n",
    "    #train random forest with starting parameters:\n",
    "    regr = RandomForestRegressor(**rf_starting_params)\n",
    "    \n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_preds = regr.predict(X_train)\n",
    "    y_test_preds = regr.predict(X_test)\n",
    "    \n",
    "    X_train_mse = mean_squared_error(y_train.values, y_train_preds)\n",
    "    X_test_mse = mean_squared_error(y_test.values, y_test_preds)\n",
    "    \n",
    "    days_before_df.loc[len(days_before_df)] = pd.Series({'days before': n,\n",
    "                                                         'train MSE': X_train_mse,\n",
    "                                                         'test MSE': X_test_mse\n",
    "                                                         })\n",
    "    \n",
    "#calculate lowest train mse\n",
    "best_lookback_mse = days_before_df['train MSE'].min()\n",
    "        \n",
    "#get the entire row with best train mse\n",
    "best_row = days_before_df[days_before_df['train MSE'] == best_lookback_mse]\n",
    "\n",
    "best_row"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f705dfaaf2ca08c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding the best hyperparameter set based on the lowest test MSE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924a2e6f7de60ddf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create best 'n' lookback features\n",
    "best_n = int(best_row['days before'].iloc[0])\n",
    "raw_and_missing_new = preprocess.create_features(raw_and_missing, best_n, 'close_price', '_days_ago', 'tomorrow')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579503091c706a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop all missing dates - since we want to prevent our model from learning the connections between each closing price\n",
    "raw_and_missing_new =  preprocess.drop_missing_dates(raw_and_missing_new, missing)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88b0012cdaf5db12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Temporal splitting to train and test:\n",
    "split_date = preprocess.generate_split_date(raw_and_missing_new) # train size was set to 0.8 of data size\n",
    "\n",
    "train = raw_and_missing_new.iloc[raw_and_missing_new.index <= split_date].copy()\n",
    "test = raw_and_missing_new.loc[raw_and_missing_new.index > split_date].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25fe654296c817ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove rows and split the data to X and y:\n",
    "X_train, y_train = preprocess.rmv_nans_splt_Xy(train, 'tomorrow')\n",
    "X_test, y_test = preprocess.rmv_nans_splt_Xy(test, 'tomorrow')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c8d88c000a233a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running random forest on all permutations of selected parameters:\n",
    "\n",
    "activate_model = Ma.ModelActivator()\n",
    "\n",
    "params_and_mse_df = pd.DataFrame(columns=['parameters set', 'train MSE', 'test MSE'])\n",
    "\n",
    "rf_full_params = {'n_estimators': [50, 100, 200],  # The number of trees in the forest\n",
    "                'max_depth': [None, 5, 7, 9], # The maximum depth of the tree.\n",
    "                                            # If None, then nodes are expanded until all leaves are pure or until all leaves contain less   than min_samples_split samples\n",
    "                'min_samples_split': [2, 10, 20, 30],\n",
    "                'max_features': [1, 3, 5, 7],  # The number of features to consider when looking for the best split results\n",
    "                'oob_score': [True, False],\n",
    "                'random_state': [None, 0, 711] # The randomness of the bootstrapping of the samples used when building trees and                                the sampling of the features to consider when looking for the best split at each node\n",
    "                }\n",
    "\n",
    "all_rf_params_permutations = list(ParameterGrid(rf_full_params))\n",
    "    \n",
    "# find the best parameters with 15 additional lookback features based on lowest test mse\n",
    "for params in tqdm(all_rf_params_permutations, desc=\"model params\"):\n",
    "\n",
    "# train model, return train mse and test mse\n",
    "    train_mse, test_mse = activate_model.run_random_forest(X_train, y_train, X_test, y_test, params)\n",
    "    \n",
    "#add all trained combinations of parameters to a dataframe along with their train mse and test mse     \n",
    "    params_and_mse_df.loc[len(params_and_mse_df)] = pd.Series({\n",
    "                                                                        'parameters set': params,\n",
    "                                                                        'train MSE': train_mse,\n",
    "                                                                        'test MSE': test_mse\n",
    "                                                                    })\n",
    "#get best test mse, which is the lowest test mse\n",
    "best_test_mse = params_and_mse_df['test MSE'].min()\n",
    "\n",
    "#get the entire row with best test mse\n",
    "best_row = params_and_mse_df[params_and_mse_df['test MSE'] == best_test_mse]\n",
    "\n",
    "best_row"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7038de6d9430c3b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params = best_row['parameters set'].iloc[0]\n",
    "best_params"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea256b12ee3d1f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training On one stock data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18a1bf8b46675d91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" PREPROCESSING: remove rows and split the data to X and y. \"\"\"\n",
    "X_data, y_data = preprocess.rmv_nans_splt_Xy(raw_and_missing_new, 'tomorrow')\n",
    "\n",
    "\"\"\"train model on data with best hyperparameter set\"\"\"\n",
    "regr = RandomForestRegressor(**best_params)\n",
    "    \n",
    "regr.fit(X_data, y_data)\n",
    "\n",
    "\"\"\"save trained model\"\"\"\n",
    "joblib.dump(regr, \"./rf_compressed.joblib\", compress=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13f0df46f4fb338"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "export last date+price of one stock to csv:\n",
    "\"\"\"\n",
    "stock_name = stocks_by_id.loc[stocks_by_id.loc[:,'id'] == 1].values[0][1]\n",
    "last_date_price = raw_and_missing_new.iloc[-1,0]\n",
    "last_date_df = pd.DataFrame(data=[[stock_name ,last_date_price]], columns=['stock ticker', 'close price'])\n",
    "last_date_df.to_csv('./latest date one stock.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "772e2c3606b4ae2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b33b3378c2e7857b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running random forest on multiple options of 'days before'(= n):\n",
    "\n",
    "activate_model = Ma.ModelActivator()\n",
    "\n",
    "days_before_list = list(range(1, 16))\n",
    "\n",
    "features_names_list = []\n",
    "\n",
    "all_params_and_mse_df = pd.DataFrame(columns=['parameters set', 'feature importances', 'train mse', 'test mse'])\n",
    "\n",
    "best_params_and_mse_df = pd.DataFrame(columns=['days before', 'feature importances', 'parameters set', 'train mse', 'test mse'])\n",
    "\n",
    "rf_full_params = {'n_estimators': [50, 100, 200],  # The number of trees in the forest\n",
    "                'max_depth': [None, 5, 7, 9], # The maximum depth of the tree.\n",
    "                                            # If None, then nodes are expanded until all leaves are pure or until all leaves contain less   than min_samples_split samples\n",
    "                'min_samples_split': [2, 10, 20, 30],\n",
    "                'max_features': [1, 3, 5, 7],  # The number of features to consider when looking for the best split results\n",
    "                'random_state': [None, 0, 711] # The randomness of the bootstrapping of the samples used when building trees and                                the sampling of the features to consider when looking for the best split at each node\n",
    "                }\n",
    "\n",
    "all_rf_params_permutations = list(ParameterGrid(rf_full_params))\n",
    "\n",
    "for n in tqdm(days_before_list, desc='lookback days'):\n",
    "    \n",
    "    # create lookback features\n",
    "    raw_and_missing_new = preprocess.create_features(raw_and_missing, n, 'close_price', '_days_ago', 'tomorrow')\n",
    "    \n",
    "    # drop all missing dates - since we want to prevent our model from learning the connections between each closing price\n",
    "    raw_and_missing_new =  preprocess.drop_missing_dates(raw_and_missing_new, missing)\n",
    "\n",
    "    ## Temporal splitting to train and test:\n",
    "    split_date = preprocess.generate_split_date(raw_and_missing_new) # train size was set to 0.8 of data size\n",
    "\n",
    "    train = raw_and_missing_new.iloc[raw_and_missing_new.index <= split_date].copy()\n",
    "    test = raw_and_missing_new.loc[raw_and_missing_new.index > split_date].copy()\n",
    "\n",
    "    # remove rows and split the data to X and y:\n",
    "    X_train, y_train = preprocess.rmv_nans_splt_Xy(train, 'tomorrow')\n",
    "    X_test, y_test = preprocess.rmv_nans_splt_Xy(test, 'tomorrow')\n",
    "    \n",
    "    # find best parameters for every 'n' number of days before based on lowest test mse\n",
    "    for params in tqdm(all_rf_params_permutations, desc=\"model params\"):\n",
    "    \n",
    "    # train model, return train mse, test mse and feature importances\n",
    "        train_mse, test_mse, feature_importances = activate_model.run_random_forest(X_train, y_train, X_test, y_test, params)\n",
    "        \n",
    "\n",
    "    #add all trained combinations of parameters to a dataframe along with their train mse and test mse     \n",
    "        all_params_and_mse_df.loc[len(all_params_and_mse_df)] = pd.Series({\n",
    "                                                                            'parameters set':params,\n",
    "                                                                            'feature importances':feature_importances,\n",
    "                                                                            'train mse':train_mse,\n",
    "                                                                            'test mse':test_mse\n",
    "                                                                        })\n",
    "    #get best test mse, which is the lowest test mse\n",
    "    best_test_mse = all_params_and_mse_df['test mse'].min()\n",
    "    \n",
    "    #get the entire row with best test mse\n",
    "    best_row = all_params_and_mse_df[all_params_and_mse_df['test mse'] == best_test_mse]\n",
    "    \n",
    "    #put the lookback days along with the best row in a dataframe \n",
    "    best_params_and_mse_df.loc[len(best_params_and_mse_df)] = pd.Series({\n",
    "                                                                'days before':n,\n",
    "                                                                'parameters set':best_row['parameters set'].values[0],\n",
    "                                                                'feature importances':best_row['feature importances'].values,\n",
    "                                                                'train mse':float(best_row['train mse'].values),\n",
    "                                                                'test mse':float(best_row['test mse'].values)\n",
    "                                                                })\n",
    "    all_params_and_mse_df = pd.DataFrame(columns=['parameters set', 'feature importances', 'train mse', 'test mse'])\n",
    "    features_names_list.append(X_test.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8cfad3bea141e8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#results for 1, 3, 5, 7 lookback features\n",
    "best_params_and_mse_df[['days before', 'train mse', 'test mse']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c0840c970c57e57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(days_before_list)):\n",
    "    forest_importances_df = pd.DataFrame(best_params_and_mse_df['feature importances'][i][0], index=features_names_list[i])\n",
    "    forest_importances_df.plot.bar(legend=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "732fb771265d9d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(days_before_list)):\n",
    "    forest_importances_df = pd.DataFrame(best_params_and_mse_df['feature importances'][i][0], index=features_names_list[i])\n",
    "    forest_importances_df.plot.bar(legend=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45133e8d2157b20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#results for 7, 17, 27 lookback features\n",
    "best_params_and_mse_df[['days before', 'train mse', 'test mse']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7ad607713a96511"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
